{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, you will learn how to apply machine learning techniques to predict a passenger's chance of surviving using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "## Intro to Decision Trees\n",
    "In the previous chapter, you did all the slicing and dicing yourself to find subsets that have a higher chance of surviving. A decision tree automates this process for you and outputs a classification model or classifier.\n",
    "\n",
    "Conceptually, the decision tree algorithm starts with all the data at the root node and scans all the variables for the best one to split on. Once a variable is chosen, you do the split and go down one level (or one node) and repeat. The final nodes at the bottom of the decision tree are known as terminal nodes, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "# Load the train and test datasets to create two DataFrames\n",
    "# you can import from URL in this way:\n",
    "#\n",
    "#\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Import necessary packages\n",
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None  \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and Formatting your Data\n",
    "Before you can begin constructing your trees you need to get your hands dirty and clean the data so that you can use all the features available to you. In the first chapter, we saw that the Age variable had some missing value. Missingness is a whole subject with and in itself, but we will use a simple imputation technique where we substitute each missing value with the median of the all present values.\n",
    "\n",
    "*train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())*\n",
    "\n",
    "Another problem is that the Sex and Embarked variables are categorical but in a non-numeric format. Thus, we will need to assign each class a unique integer so that Python can handle the information. Embarked also has some missing values which you should impute witht the most common class of embarkation, which is \"S\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the male and female groups to integer form\n",
    "\n",
    "train_one = train.copy()\n",
    "\n",
    "train_one[\"Sex\"][train_one[\"Sex\"] == \"male\"] = 0\n",
    "train_one[\"Sex\"][train_one[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Impute Age variable\n",
    "train_one[\"Age\"] = train_one[\"Age\"].fillna(train_one[\"Age\"].median())\n",
    "\n",
    "# Impute the Embarked variable\n",
    "train_one[\"Embarked\"] = train_one[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train_one[\"Embarked\"][train_one[\"Embarked\"] == \"S\"] = 0\n",
    "train_one[\"Embarked\"][train_one[\"Embarked\"] == \"C\"] = 1\n",
    "train_one[\"Embarked\"][train_one[\"Embarked\"] == \"Q\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "5      0\n",
      "6      0\n",
      "7      0\n",
      "8      1\n",
      "9      1\n",
      "10     1\n",
      "11     1\n",
      "12     0\n",
      "13     0\n",
      "14     1\n",
      "15     1\n",
      "16     0\n",
      "17     0\n",
      "18     1\n",
      "19     1\n",
      "20     0\n",
      "21     0\n",
      "22     1\n",
      "23     0\n",
      "24     1\n",
      "25     1\n",
      "26     0\n",
      "27     0\n",
      "28     1\n",
      "29     0\n",
      "      ..\n",
      "861    0\n",
      "862    1\n",
      "863    1\n",
      "864    0\n",
      "865    1\n",
      "866    1\n",
      "867    0\n",
      "868    0\n",
      "869    0\n",
      "870    0\n",
      "871    1\n",
      "872    0\n",
      "873    0\n",
      "874    1\n",
      "875    1\n",
      "876    0\n",
      "877    0\n",
      "878    0\n",
      "879    1\n",
      "880    1\n",
      "881    0\n",
      "882    1\n",
      "883    0\n",
      "884    0\n",
      "885    1\n",
      "886    0\n",
      "887    1\n",
      "888    1\n",
      "889    0\n",
      "890    0\n",
      "Name: Sex, Length: 891, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the Sex and Embarked columns\n",
    "print(train_one[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "5      2\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "9      1\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13     0\n",
      "14     0\n",
      "15     0\n",
      "16     2\n",
      "17     0\n",
      "18     0\n",
      "19     1\n",
      "20     0\n",
      "21     0\n",
      "22     2\n",
      "23     0\n",
      "24     0\n",
      "25     0\n",
      "26     1\n",
      "27     0\n",
      "28     2\n",
      "29     0\n",
      "      ..\n",
      "861    0\n",
      "862    0\n",
      "863    0\n",
      "864    0\n",
      "865    0\n",
      "866    1\n",
      "867    0\n",
      "868    0\n",
      "869    0\n",
      "870    0\n",
      "871    0\n",
      "872    0\n",
      "873    0\n",
      "874    1\n",
      "875    1\n",
      "876    0\n",
      "877    0\n",
      "878    0\n",
      "879    1\n",
      "880    0\n",
      "881    0\n",
      "882    0\n",
      "883    0\n",
      "884    0\n",
      "885    2\n",
      "886    0\n",
      "887    0\n",
      "888    0\n",
      "889    1\n",
      "890    2\n",
      "Name: Embarked, Length: 891, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_one[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating your first decision tree\n",
    "You will use the scikit-learn and numpy libraries to build your first decision tree. scikit-learn can be used to create tree objects from the DecisionTreeClassifier class. The methods that we will use take numpy arrays as inputs and therefore we will need to create those from the DataFrame that we already have. We will need the following to build a decision tree\n",
    "\n",
    "target: A one-dimensional numpy array containing the target/response from the train data. (Survival in your case)\n",
    "features: A multidimensional numpy array containing the features/predictors from the train data. (ex. Sex, Age)\n",
    "Take a look at the sample code below to see what this would look like:\n",
    "\n",
    "###### *target = train[\"Survived\"].values*\n",
    "\n",
    "###### *features = train[[\"Sex\", \"Age\"]].values*\n",
    "\n",
    "###### *my_tree = tree.DecisionTreeClassifier()*\n",
    "\n",
    "###### *my_tree = my_tree.fit(features, target)*\n",
    "\n",
    "\n",
    "One way to quickly see the result of your decision tree is to see the importance of the features that are included. This is done by requesting the .feature_importances_ attribute of your tree object. Another quick metric is the mean accuracy that you can compute using the *.score()* function with features_one and target as arguments.\n",
    "\n",
    "Ok, time for you to build your first decision tree in Python! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12482906 0.31274009 0.22797589 0.33445495]\n",
      "0.9775533108866442\n"
     ]
    }
   ],
   "source": [
    "# Print the train data to see the available features\n",
    "#print(train)\n",
    "\n",
    "\n",
    "# Create the target and features numpy arrays: target, features_one\n",
    "target = train_one[\"Survived\"].values\n",
    "features_one = train_one[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and submit to Kaggle\n",
    "To send a submission to Kaggle you need to predict the survival rates for the observations in the test set. In the last exercise of the previous chapter, we created simple predictions based on a single subset. Luckily, with our decision tree, we can make use of some simple functions to \"generate\" our answer without having to manually perform subsetting.\n",
    "\n",
    "First, you make use of the .predict() method. You provide it the model (my_tree_one), the values of features from the dataset for which predictions need to be made (test). To extract the features we will need to create a numpy array in the same way as we did when training the model. However, we need to take care of a small but important problem first. There is a missing value in the Fare feature that needs to be imputed.\n",
    "\n",
    "Next, you need to make sure your output is in line with the submission requirements of Kaggle: a csv file with exactly 418 entries and two columns: PassengerId and Survived. Then use the code provided to make a new data frame using DataFrame(), and create a csv file using to_csv() method from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing value with the median\n",
    "test_one = test.copy()\n",
    "test_one.Fare[152] = test_one.Fare.median()\n",
    "\n",
    "test_one[\"Sex\"][test_one[\"Sex\"] == \"male\"] = 0\n",
    "test_one[\"Sex\"][test_one[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "# Impute Age variable\n",
    "test_one[\"Age\"] = test_one[\"Age\"].fillna(test_one[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
      " 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0\n",
      " 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 0]\n",
      "      Survived\n",
      "892          0\n",
      "893          0\n",
      "894          1\n",
      "895          1\n",
      "896          1\n",
      "897          0\n",
      "898          0\n",
      "899          0\n",
      "900          1\n",
      "901          0\n",
      "902          0\n",
      "903          0\n",
      "904          1\n",
      "905          1\n",
      "906          1\n",
      "907          1\n",
      "908          0\n",
      "909          1\n",
      "910          1\n",
      "911          0\n",
      "912          0\n",
      "913          1\n",
      "914          1\n",
      "915          1\n",
      "916          1\n",
      "917          0\n",
      "918          1\n",
      "919          1\n",
      "920          1\n",
      "921          0\n",
      "...        ...\n",
      "1280         0\n",
      "1281         0\n",
      "1282         0\n",
      "1283         1\n",
      "1284         1\n",
      "1285         0\n",
      "1286         0\n",
      "1287         1\n",
      "1288         0\n",
      "1289         1\n",
      "1290         0\n",
      "1291         0\n",
      "1292         1\n",
      "1293         0\n",
      "1294         1\n",
      "1295         1\n",
      "1296         0\n",
      "1297         0\n",
      "1298         0\n",
      "1299         0\n",
      "1300         1\n",
      "1301         1\n",
      "1302         1\n",
      "1303         1\n",
      "1304         0\n",
      "1305         0\n",
      "1306         1\n",
      "1307         0\n",
      "1308         0\n",
      "1309         0\n",
      "\n",
      "[418 rows x 1 columns]\n",
      "(418, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "test_features = test_one[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n",
    "\n",
    "# Make your prediction using the test set and print them.\n",
    "my_prediction = my_tree_one.predict(test_features)\n",
    "print(my_prediction)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "PassengerId =np.array(test[\"PassengerId\"]).astype(int)\n",
    "my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [\"Survived\"])\n",
    "print(my_solution)\n",
    "\n",
    "# Check that your data frame has 418 entries\n",
    "print(my_solution.shape)\n",
    "\n",
    "# Write your solution to a csv file with the name my_solution.csv\n",
    "my_solution.to_csv(\"my_solution_one.csv\", index_label = [\"PassengerId\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting and how to control it\n",
    "When you created your first decision tree the default arguments for max_depth and min_samples_split were set to None. This means that no limit on the depth of your tree was set. That's a good thing right? Not so fast. We are likely overfitting. This means that while your model describes the training data extremely well, it doesn't generalize to new data, which is frankly the point of prediction. Just look at the Kaggle submission results for the simple model based on Gender and the complex decision tree. Which one does better?\n",
    "\n",
    "Maybe we can improve the overfit model by making a less complex model? In DecisionTreeRegressor, the depth of our model is defined by two parameters:\n",
    "\n",
    "* the *max_depth* parameter determines when the splitting up of the decision tree stops.\n",
    "* the *min_samples_split* parameter monitors the amount of observations in a bucket. If a certain threshold is not reached (e.g minimum 10 passengers) no further splitting can be done.\n",
    "\n",
    "By limiting the complexity of your decision tree you will increase its generality and thus its usefulness for prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9057239057239057\n"
     ]
    }
   ],
   "source": [
    "# Create a new array with the added features: features_two\n",
    "features_two = train_one[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "#Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5 : my_tree_two\n",
    "max_depth = 10\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)\n",
    "my_tree_two = my_tree_two.fit(features_two, target)\n",
    "\n",
    "#Print the score of the new decison tree\n",
    "print(my_tree_two.score(features_two, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature-engineering for our Titanic data set\n",
    "Data Science is an art that benefits from a human element. Enter feature engineering: creatively engineering your own features by combining the different existing variables.\n",
    "\n",
    "While feature engineering is a discipline in itself, too broad to be covered here in detail, you will have a look at a simple example by creating your own new predictive attribute: *family_size*.\n",
    "\n",
    "A valid assumption is that larger families need more time to get together on a sinking ship, and hence have lower probability of surviving. Family size is determined by the variables *SibSp* and *Parch*, which indicate the number of family members a certain passenger is traveling with. So when doing feature engineering, you add a new variable *family_size*, which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9797979797979798\n"
     ]
    }
   ],
   "source": [
    "# Create train_two with the newly defined feature\n",
    "train_two = train_one.copy()\n",
    "train_two[\"family_size\"] = train_two.SibSp + train_two.Parch+1\n",
    "\n",
    "# Create a new feature set and add the new feature\n",
    "features_three = train_two[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", 'family_size']].values\n",
    "\n",
    "# Define the tree classifier, then fit the model\n",
    "my_tree_three = tree.DecisionTreeClassifier()\n",
    "my_tree_three = my_tree_three.fit(features_three, target)\n",
    "\n",
    "# Print the score of this decision tree\n",
    "print(my_tree_three.score(features_three, target))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
